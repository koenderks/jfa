<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Algorithmic fairness • jfa</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Algorithmic fairness">
<meta property="og:description" content="jfa">
<meta property="og:image" content="https://koenderks.github.io/jfa/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">jfa</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.7.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/jfa.html">Get started</a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/koenderks/jfa" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/koenderks/jfa/discussions" class="external-link">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Algorithmic fairness</h1>
                        <h4 data-toc-skip class="author">Koen Derks</h4>
            
            <h4 data-toc-skip class="date">2023-07-22</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/koenderks/jfa/blob/HEAD/vignettes/model-fairness.Rmd" class="external-link"><code>vignettes/model-fairness.Rmd</code></a></small>
      <div class="hidden name"><code>model-fairness.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Welcome to the ‘Algorithmic fairness’ vignette of the
<strong>jfa</strong> package. In this vignette you can find a detailed
example of how you can use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function
provided by the package.</p>
</div>
<div class="section level2">
<h2 id="function-model_fairness">Function: <code>model_fairness()</code><a class="anchor" aria-label="anchor" href="#function-model_fairness"></a>
</h2>
<p>The <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function provides a method to
assess fairness in algorithmic decision-making systems by computing
various model-agnostic metrics based on the observed and predicted
labels in a data set. Calculated metrics include demographic parity,
proportional parity, predictive rate parity, accuracy parity, false
negative rate parity, false positive rate parity, true positive rate
parity, negative predicted value parity, and specificity parity. Other
than computing these metrics, the function helps to decide whether
groups are treated fairly to a certain degree and within a certain
materiality threshold.</p>
<p>Note that, in an audit context, not all fairness measures are equally
appropriate in all situations. The decision tree below aids with
choosing which measure is best for the situation at hand (Büyük,
2023).</p>
<p align="center">
<img src="fairness-tree.png" alt="fairness" width="80%"></p>
<p><em>Example:</em></p>
<p>To illustrate how to use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function,
we will use a well-known data set called COMPAS. The COMPAS
(Correctional Offender Management Profiling for Alternative Sanctions)
software is is a case management and decision support tool used by some
U.S. courts to assess the likelihood of a defendant becoming a
recidivist (repeated offender).</p>
<p>The <code>compas</code> data is included in the package and contains
predictions of the COMPAS software for several cases. The data can be
loaded with <code>data("compas")</code> and contains for each defendant,
whether the defendant did commit a crime within two years after the
court case (<code>TwoYrRecidivism</code>), some personal characteristics
like gender and ethnicity, and whether the software predicted the
defendant to be a recidivist (<code>Predicted</code>).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"compas"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">compas</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    TwoYrRecidivism AgeAboveFoutryFive AgeBelowTwentyFive Gender Misdemeanor</span></span>
<span><span class="co">## 4               no                 no                 no   Male         yes</span></span>
<span><span class="co">## 5              yes                 no                 no   Male          no</span></span>
<span><span class="co">## 7               no                 no                 no Female         yes</span></span>
<span><span class="co">## 11              no                 no                 no   Male          no</span></span>
<span><span class="co">## 14              no                 no                 no   Male         yes</span></span>
<span><span class="co">## 24              no                 no                 no   Male         yes</span></span>
<span><span class="co">##           Ethnicity Predicted</span></span>
<span><span class="co">## 4             Other        no</span></span>
<span><span class="co">## 5         Caucasian       yes</span></span>
<span><span class="co">## 7         Caucasian        no</span></span>
<span><span class="co">## 11 African_American        no</span></span>
<span><span class="co">## 14         Hispanic        no</span></span>
<span><span class="co">## 24            Other        no</span></span></code></pre>
<p>We will investigate whether the algorithm is fair with respect to the
sensitive attribute race. Considering the context, a positive prediction
means that a defendant is classified as a reoffender, and a negative
prediction means that a defendant is classified as a non-reoffender. The
fairness metrics offer information on whether there are any disparities
in the algorithm’s predictions across different ethnic groups. By
calculating and reviewing these metrics, we can get an indication of
whether the algorithm exhibits any discriminatory behavior towards
specific ethnic groups. If substantial disparities exist, we may need to
investigate further and potentially modify the algorithm to ensure
fairness in its predictions.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"compas"</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span><span class="va">compas</span>, <span class="st">"Ethnicity"</span>, <span class="st">"TwoYrRecidivism"</span>, <span class="st">"Predicted"</span>, reference <span class="op">=</span> <span class="st">"Caucasian"</span>, positive <span class="op">=</span> <span class="st">"yes"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Algorithmic Fairness Metrics Summary</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Reference group:  Caucasian</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Confusion matrix:</span></span>
<span><span class="co">##       Predicted</span></span>
<span><span class="co">## Actual  no yes</span></span>
<span><span class="co">##    no  997 284</span></span>
<span><span class="co">##    yes 434 388</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model performance:</span></span>
<span><span class="co">##             Caucasian African_American   Asian Hispanic Native_American   Other</span></span>
<span><span class="co">##   Support        2103             3175      31      509              11     343</span></span>
<span><span class="co">##   Accuracy    0.65858          0.67244 0.74194  0.68173         0.63636 0.69388</span></span>
<span><span class="co">##   Precision   0.57738          0.66525     0.5   0.5906             0.6 0.61176</span></span>
<span><span class="co">##   Recall      0.47202          0.75256    0.25  0.46561             0.6 0.41935</span></span>
<span><span class="co">##   F1 score    0.51941          0.70621 0.33333  0.52071             0.6 0.49761</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Fairness metrics (parity ratio):</span></span>
<span><span class="co">##                                     Caucasian  African_American</span></span>
<span><span class="co">##   Demographic parity                  672 (1)     1879 (2.7961)</span></span>
<span><span class="co">##   Proportional parity             0.31954 (1)  0.59181 (1.8521)</span></span>
<span><span class="co">##   Predictive rate parity          0.57738 (1)  0.66525 (1.1522)</span></span>
<span><span class="co">##   Accuracy parity                 0.65858 (1)   0.67244 (1.021)</span></span>
<span><span class="co">##   False negative rate parity      0.52798 (1) 0.24744 (0.46866)</span></span>
<span><span class="co">##   False positive rate parity       0.2217 (1)  0.41546 (1.8739)</span></span>
<span><span class="co">##   True positive rate parity       0.47202 (1)  0.75256 (1.5943)</span></span>
<span><span class="co">##   Negative predicted value parity 0.69672 (1) 0.68287 (0.98013)</span></span>
<span><span class="co">##   Specificity parity               0.7783 (1) 0.58454 (0.75105)</span></span>
<span><span class="co">##   Outside tolerance region                                    6</span></span>
<span><span class="co">##                                                Asian          Hispanic</span></span>
<span><span class="co">##   Demographic parity                   4 (0.0059524)     149 (0.22173)</span></span>
<span><span class="co">##   Proportional parity               0.12903 (0.4038) 0.29273 (0.91609)</span></span>
<span><span class="co">##   Predictive rate parity               0.5 (0.86598)   0.5906 (1.0229)</span></span>
<span><span class="co">##   Accuracy parity                   0.74194 (1.1266)  0.68173 (1.0351)</span></span>
<span><span class="co">##   False negative rate parity           0.75 (1.4205)  0.53439 (1.0121)</span></span>
<span><span class="co">##   False positive rate parity      0.086957 (0.39222) 0.19062 (0.85983)</span></span>
<span><span class="co">##   True positive rate parity           0.25 (0.52964) 0.46561 (0.98642)</span></span>
<span><span class="co">##   Negative predicted value parity   0.77778 (1.1163)  0.71944 (1.0326)</span></span>
<span><span class="co">##   Specificity parity                0.91304 (1.1731)  0.80937 (1.0399)</span></span>
<span><span class="co">##   Outside tolerance region                         5                 1</span></span>
<span><span class="co">##                                     Native_American             Other</span></span>
<span><span class="co">##   Demographic parity                  5 (0.0074405)      85 (0.12649)</span></span>
<span><span class="co">##   Proportional parity              0.45455 (1.4225) 0.24781 (0.77552)</span></span>
<span><span class="co">##   Predictive rate parity               0.6 (1.0392)  0.61176 (1.0596)</span></span>
<span><span class="co">##   Accuracy parity                 0.63636 (0.96626)  0.69388 (1.0536)</span></span>
<span><span class="co">##   False negative rate parity           0.4 (0.7576)  0.58065 (1.0997)</span></span>
<span><span class="co">##   False positive rate parity       0.33333 (1.5035) 0.15068 (0.67967)</span></span>
<span><span class="co">##   True positive rate parity            0.6 (1.2711) 0.41935 (0.88843)</span></span>
<span><span class="co">##   Negative predicted value parity 0.66667 (0.95687)  0.72093 (1.0348)</span></span>
<span><span class="co">##   Specificity parity              0.66667 (0.85657)  0.84932 (1.0912)</span></span>
<span><span class="co">##   Outside tolerance region                        5                 3</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p><img src="model-fairness_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;"></p>
<p>Let’s interpret the fairness metrics for the African American, Asian,
and Hispanic groups in comparison to the reference group
(Caucasian).</p>
<ol style="list-style-type: decimal">
<li>
<strong>Demographic parity (Statistical parity)</strong>: Compares
the number of positive predictions (e.g., reoffenders) between each
ethnic group and the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The ratio of positive predictions for
African Americans compared to Caucasians is 2.7961, indicating that
there are nearly three times more African Americans predicted as
reoffenders than Caucasians.</li>
<li>
<em>Asian</em>: The ratio for Asians is very close to zero
(0.0059524), indicating that there are many less Asians (4) that are
predicted as reoffenders in these data than there are Caucasians.</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 0.22173, meaning that
there are about five times less Hispanics predicted as reoffenders than
that there are Caucasians.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Proportional parity (Disparate impact)</strong>: Compares
the positive prediction rates (e.g., true positive rate) of each ethnic
group with the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The ratio of true positive rates (TPRs)
for African Americans compared to Caucasians is 1.8521. This indicates
that the TPR for African Americans is approximately 1.85 times higher
than for Caucasians. Again, this suggests potential bias in the
algorithm’s predictions against African Americans.</li>
<li>
<em>Asian</em>: The proportional parity ratio for Asians is 0.4038,
indicating that their TPR is lower than for Caucasians. This suggests
potential underestimation of reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 0.91609, suggesting
that their TPR is close to the reference group (Caucasians). This
indicates relatively fair treatment of Hispanics in the algorithm’s
predictions.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Predictive rate parity (Equalized odds)</strong>: Compares
the overall positive prediction rates (e.g., reoffender prediction) of
different ethnic groups with the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The predictive rate parity ratio for
African Americans is 1.1522. This suggests that the overall positive
prediction rate for African Americans is approximately 1.15 times higher
than for Caucasians. This indicates potential favoritism towards African
Americans in the overall positive predictions made by the
algorithm.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.86598, indicating that
their overall positive prediction rate is lower than for Caucasians.
This suggests potential underestimation of reoffenders among Asians by
the algorithm.</li>
<li>
<em>Hispanic</em>: The predictive rate parity ratio for Hispanics is
1.0229, suggesting their overall positive prediction rate is very close
to that of the reference group (Caucasians). This indicates relatively
fair treatment in the algorithm’s overall positive predictions.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>
<strong>Accuracy parity</strong>: Compares the accuracy of each
ethnic group’s predictions with the reference group.</li>
</ol>
<ul>
<li>African American: The accuracy parity ratio for African Americans is
1.021, suggesting their accuracy is very similar to the reference group
(Caucasians). This indicates fair treatment concerning overall
accuracy.</li>
<li>
<em>Asian</em>: The accuracy parity ratio for Asians is 1.1266,
suggesting their accuracy is slightly higher than for Caucasians,
indicating potential favoritism in overall accuracy.</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 1.0351, suggesting
their accuracy is slightly higher than for Caucasians, indicating
potential favoritism in overall accuracy.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>
<strong>False negative rate parity (Treatment equality)</strong>:
Compares the false negative rates (e.g., for reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The ratio of false negative rates (FNRs)
for African Americans compared to Caucasians is 0.46866. A value lower
than 1 suggests that African Americans are less likely to be falsely
classified as non-reoffenders, indicating potential bias against this
group in this aspect.</li>
<li>
<em>Asian</em>: The ratio for Asians is 1.4205, indicating that they
are more likely to be falsely classified as non-reoffenders compared to
Caucasians, suggesting potential underestimation of reoffenders among
Asians.</li>
<li>
<em>Hispanic</em>: The FNR parity ratio for Hispanics is 1.0121,
indicating relatively similar rates as the reference group (Caucasians),
suggesting fair treatment in this aspect.</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>
<strong>False positive rate parity</strong>: Compares the false
positive rates (e.g., for non-reoffenders) of each ethnic group with the
reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The false positive rate parity ratio for
African Americans is 1.8739. This indicates that African Americans are
approximately 1.87 times more likely to be falsely predicted as
reoffenders than Caucasians. This suggests potential bias in the
algorithm’s false positive predictions in favor of African
Americans.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.39222, indicating that
they are less likely to be falsely predicted as reoffenders compared to
Caucasians. This suggests potential fair treatment of Asians in false
positive predictions.</li>
<li>
<em>Hispanic</em>: The false positive rate parity ratio for
Hispanics is 0.85983, suggesting they are less likely to be falsely
predicted as reoffenders compared to Caucasians. This indicates
potential fair treatment of Hispanics in false positive
predictions.</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>
<strong>True positive rate parity (Equal opportunity)</strong>:
Compares the true positive rates (e.g., for reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The true positive rate parity ratio for
African Americans is 1.5943. This indicates that African Americans are
approximately 1.59 times more likely to be correctly predicted as
reoffenders than Caucasians. This suggests potential favoritism towards
African Americans in true positive predictions made by the
algorithm.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.52964, indicating that
they are less likely to be correctly predicted as reoffenders compared
to Caucasians. This suggests potential underestimation of reoffenders
among Asians by the algorithm.</li>
<li>
<em>Hispanic:</em> The true positive rate parity ratio for Hispanics
is 0.98642, suggesting their true positive rate is very close to that of
the reference group (Caucasians). This indicates relatively fair
treatment in the algorithm’s true positive predictions.</li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li>
<strong>Negative predicted value parity</strong>: Compares the
negative predicted values (e.g., for non-reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The Negative predicted value (NPV) parity
ratio for African Americans is 0.98013. A value close to 1 indicates
that the NPV for African Americans is very similar to the reference
group (Caucasians). This suggests fair treatment in predicting
non-reoffenders among African Americans.</li>
<li>
<em>Asian</em>: The NPV parity ratio for Asians is 1.1163,
indicating that their NPV is slightly higher than for Caucasians. This
could suggest potential favoritism towards Asians in predicting
non-reoffenders.</li>
<li>
<em>Hispanic</em>: The NPV parity ratio for Hispanics is 1.0326,
suggesting that their NPV is slightly higher than for Caucasians. This
indicates potential favoritism towards Hispanics in predicting
non-reoffenders.</li>
</ul>
<ol start="9" style="list-style-type: decimal">
<li>
<strong>Specificity parity (True negative rate parity)</strong>:
Compares the specificity (true negative rate) of each ethnic group with
the reference group.</li>
</ol>
<ul>
<li>
<em>African American</em>: The specificity parity ratio for African
Americans is 0.75105. A value lower than 1 indicates that the
specificity for African Americans is lower than for Caucasians. This
suggests potential bias in correctly identifying non-reoffenders among
African Americans.</li>
<li>
<em>Asian</em>: The specificity parity ratio for Asians is 1.1731,
indicating their specificity is slightly higher than for Caucasians.
This could suggest potential favoritism in correctly identifying
non-reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The specificity parity ratio for Hispanics is
1.0399, suggesting that their specificity is very close to the reference
group (Caucasians). This indicates relatively fair treatment in
correctly identifying non-reoffenders among Hispanics.</li>
</ul>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li>Büyük, S. (2023). <em>Automatic Fairness Criteria and Fair Model
Selection for Critical ML Tasks</em>, Master Thesis, Utrecht University.
- <a href="https://studenttheses.uu.nl/handle/20.500.12932/44225" class="external-link">View
Online</a>
</li>
<li>Pessach, D. &amp; Shmueli, E. (2022). A review on fairness in
machine learning. <em>ACM Computing Surveys</em>, 55(3), 1-44. - <a href="https://doi.org/10.1145/3494672" class="external-link">View Online</a>
</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://koenderks.com" class="external-link">Koen Derks</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
