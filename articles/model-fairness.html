<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Algorithmic fairness • jfa</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Algorithmic fairness">
<meta property="og:description" content="jfa">
<meta property="og:image" content="https://koenderks.github.io/jfa/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">jfa</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.7.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/jfa.html">Get started</a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/koenderks/jfa" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/koenderks/jfa/discussions" class="external-link">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Algorithmic fairness</h1>
                        <h4 data-toc-skip class="author">Koen Derks</h4>
            
            <h4 data-toc-skip class="date">2023-08-05</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/koenderks/jfa/blob/HEAD/vignettes/model-fairness.Rmd" class="external-link"><code>vignettes/model-fairness.Rmd</code></a></small>
      <div class="hidden name"><code>model-fairness.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Welcome to the ‘Algorithmic fairness’ vignette of the
<strong>jfa</strong> package. In this vignette you can find a detailed
example of how you can use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function
provided by the package.</p>
</div>
<div class="section level2">
<h2 id="function-model_fairness">Function: <code>model_fairness()</code><a class="anchor" aria-label="anchor" href="#function-model_fairness"></a>
</h2>
<p>The <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function provides a method to
assess fairness in algorithmic decision-making systems by computing
various model-agnostic metrics based on the observed and predicted
labels in a data set. Calculated metrics include demographic parity,
proportional parity, predictive rate parity, accuracy parity, false
negative rate parity, false positive rate parity, true positive rate
parity, negative predicted value parity, and specificity parity.</p>
<p>Note that, in an audit context, not all fairness measures are equally
appropriate in all situations. The decision tree below aids with
choosing which measure is best for the situation at hand (Büyük,
2023).</p>
<p align="center">
<img src="fairness-tree.png" alt="fairness" width="80%"></p>
<p><em>Example:</em></p>
<p>To illustrate how to use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function,
we will use a well-known data set called COMPAS. The COMPAS
(Correctional Offender Management Profiling for Alternative Sanctions)
software is is a case management and decision support tool used by some
U.S. courts to assess the likelihood of a defendant becoming a
recidivist (repeated offender).</p>
<p>The <code>compas</code> data is included in the package and contains
predictions of the COMPAS software for several cases. The data can be
loaded with <code>data("compas")</code> and contains for each defendant,
whether the defendant did commit a crime within two years after the
court case (<code>TwoYrRecidivism</code>), some personal characteristics
like gender and ethnicity, and whether the software predicted the
defendant to be a recidivist (<code>Predicted</code>).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"compas"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">compas</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    TwoYrRecidivism AgeAboveFoutryFive AgeBelowTwentyFive Gender Misdemeanor</span></span>
<span><span class="co">## 4               no                 no                 no   Male         yes</span></span>
<span><span class="co">## 5              yes                 no                 no   Male          no</span></span>
<span><span class="co">## 7               no                 no                 no Female         yes</span></span>
<span><span class="co">## 11              no                 no                 no   Male          no</span></span>
<span><span class="co">## 14              no                 no                 no   Male         yes</span></span>
<span><span class="co">## 24              no                 no                 no   Male         yes</span></span>
<span><span class="co">##           Ethnicity Predicted</span></span>
<span><span class="co">## 4             Other        no</span></span>
<span><span class="co">## 5         Caucasian       yes</span></span>
<span><span class="co">## 7         Caucasian        no</span></span>
<span><span class="co">## 11 African_American        no</span></span>
<span><span class="co">## 14         Hispanic        no</span></span>
<span><span class="co">## 24            Other        no</span></span></code></pre>
<p>We will investigate whether the algorithm is fair with respect to the
sensitive attribute <code>Ethnicity</code>. Considering the context, a
positive prediction means that a defendant is classified as a
reoffender, and a negative prediction means that a defendant is
classified as a non-reoffender. The fairness metrics offer information
on whether there are any disparities in the algorithm’s predictions
across different ethnic groups. By calculating and reviewing these
metrics, we can get an indication of whether the algorithm exhibits any
discriminatory behavior towards specific ethnic groups. If substantial
disparities exist, we may need to investigate further and potentially
modify the algorithm to ensure fairness in its predictions.</p>
<p>Before starting, let’s briefly explain the foundation of all fairness
metrics: the confusion matrix. This matrix presents observed versus
predicted labels, shedding light on the algorithm’s prediction mistakes.
Comprising the confusion matrix are the true positives (TP), false
positives (FP), true negatives (TN), and false negatives (FN). To
illustrate, the confusion matrix pertaining to the
<code>African American</code> group is displayed below. For example,
there are 629 people in this group that are incorrectly predicted to be
a reoffender, which represents a false positive in the confusion
matrix.</p>
<table class="table">
<colgroup>
<col width="39%">
<col width="28%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th align="right"></th>
<th align="center">
<code>Predicted</code> = <code>no</code>
</th>
<th align="center">
<code>Predicted</code> = <code>yes</code>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">
<code>TwoYrRecidivism</code> = <code>no</code>
</td>
<td align="center">885 (<code>TN</code>)</td>
<td align="center">629 (<code>FP</code>)</td>
</tr>
<tr class="even">
<td align="right">
<code>TwoYrRecidivism</code> = <code>yes</code>
</td>
<td align="center">411 (<code>FN</code>)</td>
<td align="center">1250 (<code>TP</code>)</td>
</tr>
</tbody>
</table>
<p>Let’s interpret the fairness metrics for the African American, Asian,
and Hispanic groups in comparison to the reference group (Caucasian) one
by one.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Demographic parity (Statistical parity)</strong>: Compares
the number of positive predictions (e.g., reoffenders) between each
ethnic group and the reference group. Note that, since demographic
parity is not a proportion, statistical inference about its equality to
the reference group is not supported for this metric.</li>
</ol>
<p><span class="math display">\[DP = TP + FP\]</span></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"dp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: demographic parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 2.7961</span></span>
<span><span class="co">##  Asian: 0.0059524</span></span>
<span><span class="co">##  Hispanic: 0.22173</span></span>
<span><span class="co">##  Native_American: 0.0074405</span></span>
<span><span class="co">##  Other: 0.12649</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The parity ratio for African Americans
compared to Caucasians is 2.7961, indicating that there are nearly three
times more African Americans predicted as reoffenders in these data by
the algorithm than Caucasians.</li>
<li>
<em>Asian</em>: The ratio for Asians is very close to zero
(0.0059524), indicating that there are many less Asians (4) that are
predicted as reoffenders in these data than there are Caucasians
(672).</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 0.22173, meaning that
there are about five times less Hispanics predicted as reoffenders in
these data than that there are Caucasians.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Proportional parity (Disparate impact)</strong>: Compares
the positive prediction rates (e.g., true positive rate) of each ethnic
group with the reference group.</li>
</ol>
<p><span class="math display">\[PP = \frac{TP + FP}{TP + FP + TN +
FN}\]</span></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: proportional parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.8521 [1.7978, 1.9058], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##  Asian: 0.4038 [0.1136, 0.93363], p-value = 0.030318</span></span>
<span><span class="co">##  Hispanic: 0.91609 [0.79339, 1.0464], p-value = 0.26386</span></span>
<span><span class="co">##  Native_American: 1.4225 [0.52415, 2.3978], p-value = 0.3444</span></span>
<span><span class="co">##  Other: 0.77552 [0.63533, 0.92953], p-value = 0.0080834</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The ratio of true positive rates (TPRs)
for African Americans compared to Caucasians is 1.8521. This indicates
that the TPR for African Americans is approximately 1.85 times higher
than for Caucasians. Again, this suggests potential bias in the
algorithm’s predictions against African Americans. The p-value is
smaller than .05, indicating that the null hypothesis of equal
proportional parity should be rejected.</li>
<li>
<em>Asian</em>: The proportional parity ratio for Asians is 0.4038,
indicating that their TPR is lower than for Caucasians. This suggests
potential underestimation of reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 0.91609, suggesting
that their TPR is close to the reference group (Caucasians). This
indicates relatively fair treatment of Hispanics in the algorithm’s
predictions.</li>
</ul>
<p>For this (and all of the following) metrics, Bayesian inference is
supported and provides credible intervals and Bayes factors. Like in the
other functions in <code>jfa</code>, performing a Bayesian analysis
using a default prior can be achieved by setting
<code>prior = TRUE</code>. The resulting Bayes factors in favor of
rejection of the null hypothesis (<span class="math inline">\(BF_{10}\)</span>) are shown in the output
below.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span>,</span>
<span>  prior <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Bayesian Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: proportional parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.8585 [1.8155, 1.9213], BF₁₀ = 4.1528e+81</span></span>
<span><span class="co">##  Asian: 0.45845 [0.17573, 0.9353], BF₁₀ = 0.26796</span></span>
<span><span class="co">##  Hispanic: 0.92196 [0.80481, 1.0528], BF₁₀ = 0.10615</span></span>
<span><span class="co">##  Native_American: 1.4866 [0.67209, 2.3223], BF₁₀ = 0.018438</span></span>
<span><span class="co">##  Other: 0.80348 [0.65077, 0.93824], BF₁₀ = 1.815</span></span></code></pre>
<p>This is probably also a good time to show the <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code>
and <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> functions associated with the
<code><a href="../reference/model_fairness.html">model_fairness()</a></code> function. Let’s examine the frequentist
function call again, but instead of printing the output to the console,
this time we store the output in <code>x</code> and run the
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> functions on this
object.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test Summary</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Fairness metric:    Proportional parity (Disparate impact)</span></span>
<span><span class="co">## Model type:         Binary classification</span></span>
<span><span class="co">## Reference group:    Caucasian</span></span>
<span><span class="co">## Positive class:     yes </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model performance:</span></span>
<span><span class="co">##                  Support  Accuracy Precision    Recall  F1 score</span></span>
<span><span class="co">## Caucasian           2103 0.6585830 0.5773810 0.4720195 0.5194110</span></span>
<span><span class="co">## African_American    3175 0.6724409 0.6652475 0.7525587 0.7062147</span></span>
<span><span class="co">## Asian                 31 0.7419355 0.5000000 0.2500000 0.3333333</span></span>
<span><span class="co">## Hispanic             509 0.6817289 0.5906040 0.4656085 0.5207101</span></span>
<span><span class="co">## Native_American       11 0.6363636 0.6000000 0.6000000 0.6000000</span></span>
<span><span class="co">## Other                343 0.6938776 0.6117647 0.4193548 0.4976077</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates:</span></span>
<span><span class="co">##                          Proportional parity               Parity ratio</span></span>
<span><span class="co">## Caucasian (R)     0.31954 [0.29964, 0.33995]                          -</span></span>
<span><span class="co">## African_American  0.59181 [0.57448, 0.60897]    1.8521 [1.7978, 1.9058]</span></span>
<span><span class="co">## Asian            0.12903 [0.036302, 0.29834]   0.4038 [0.1136, 0.93363]</span></span>
<span><span class="co">## Hispanic          0.29273 [0.25352, 0.33437]  0.91609 [0.79339, 1.0464]</span></span>
<span><span class="co">## Native_American   0.45455 [0.16749, 0.76621]   1.4225 [0.52415, 2.3978]</span></span>
<span><span class="co">## Other             0.24781 [0.20302, 0.29702] 0.77552 [0.63533, 0.92953]</span></span>
<span><span class="co">##                                   Odds ratio    p-value</span></span>
<span><span class="co">## Caucasian (R)                              -          -</span></span>
<span><span class="co">## African_American     3.0866 [2.7453, 3.4726] &lt; 2.22e-16</span></span>
<span><span class="co">## Asian            0.31561 [0.079942, 0.91095]   0.030318</span></span>
<span><span class="co">## Hispanic           0.88138 [0.70792, 1.0939]    0.26386</span></span>
<span><span class="co">## Native_American     1.7739 [0.42669, 7.0041]     0.3444</span></span>
<span><span class="co">## Other             0.70166 [0.53338, 0.91633]  0.0080834</span></span></code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p><img src="model-fairness_files/figure-html/unnamed-chunk-6-1.png" width="700"></p>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Predictive rate parity (Equalized odds)</strong>: Compares
the overall positive prediction rates (e.g., reoffender prediction) of
different ethnic groups with the reference group.</li>
</ol>
<p><span class="math display">\[PRP = \frac{TP}{TP + FP}\]</span></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"prp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: predictive rate parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.1522 [1.1143, 1.1891], p-value = 5.4523e-05</span></span>
<span><span class="co">##  Asian: 0.86598 [0.11706, 1.6149], p-value = 1</span></span>
<span><span class="co">##  Hispanic: 1.0229 [0.87836, 1.1611], p-value = 0.78393</span></span>
<span><span class="co">##  Native_American: 1.0392 [0.25396, 1.6406], p-value = 1</span></span>
<span><span class="co">##  Other: 1.0596 [0.86578, 1.2394], p-value = 0.5621</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The predictive rate parity ratio for
African Americans is 1.1522. This suggests that the overall positive
prediction rate for African Americans is approximately 1.15 times higher
than for Caucasians. This indicates potential favoritism towards African
Americans in the overall positive predictions made by the
algorithm.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.86598, indicating that
their overall positive prediction rate is lower than for Caucasians.
This suggests potential underestimation of reoffenders among Asians by
the algorithm.</li>
<li>
<em>Hispanic</em>: The predictive rate parity ratio for Hispanics is
1.0229, suggesting their overall positive prediction rate is very close
to that of the reference group (Caucasians). This indicates relatively
fair treatment in the algorithm’s overall positive predictions.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>
<strong>Accuracy parity</strong>: Compares the accuracy of each
ethnic group’s predictions with the reference group.</li>
</ol>
<p><span class="math display">\[AP = \frac{TP + TN}{TP + FP + TN +
FN}\]</span></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"ap"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: accuracy parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.021 [0.99578, 1.0458], p-value = 0.29669</span></span>
<span><span class="co">##  Asian: 1.1266 [0.841, 1.3384], p-value = 0.44521</span></span>
<span><span class="co">##  Hispanic: 1.0351 [0.97074, 1.0963], p-value = 0.34691</span></span>
<span><span class="co">##  Native_American: 0.96626 [0.46753, 1.3525], p-value = 1</span></span>
<span><span class="co">##  Other: 1.0536 [0.975, 1.127], p-value = 0.21778</span></span></code></pre>
<ul>
<li>African American: The accuracy parity ratio for African Americans is
1.021, suggesting their accuracy is very similar to the reference group
(Caucasians). This indicates fair treatment concerning overall
accuracy.</li>
<li>
<em>Asian</em>: The accuracy parity ratio for Asians is 1.1266,
suggesting their accuracy is slightly higher than for Caucasians,
indicating potential favoritism in overall accuracy.</li>
<li>
<em>Hispanic</em>: The ratio for Hispanics is 1.0351, suggesting
their accuracy is slightly higher than for Caucasians, indicating
potential favoritism in overall accuracy.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>
<strong>False negative rate parity (Treatment equality)</strong>:
Compares the false negative rates (e.g., for reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<p><span class="math display">\[FNRP = \frac{FN}{TP + FN}\]</span></p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"fnrp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: false negative rate parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 0.46866 [0.42965, 0.50936], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##  Asian: 1.4205 [0.66128, 1.8337], p-value = 0.29386</span></span>
<span><span class="co">##  Hispanic: 1.0121 [0.87234, 1.1499], p-value = 0.93562</span></span>
<span><span class="co">##  Native_American: 0.7576 [0.099899, 1.6163], p-value = 0.67157</span></span>
<span><span class="co">##  Other: 1.0997 [0.92563, 1.2664], p-value = 0.28911</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The ratio of false negative rates (FNRs)
for African Americans compared to Caucasians is 0.46866. A value lower
than 1 suggests that African Americans are less likely to be falsely
classified as non-reoffenders, indicating potential bias against this
group in this aspect.</li>
<li>
<em>Asian</em>: The ratio for Asians is 1.4205, indicating that they
are more likely to be falsely classified as non-reoffenders compared to
Caucasians, suggesting potential underestimation of reoffenders among
Asians.</li>
<li>
<em>Hispanic</em>: The FNR parity ratio for Hispanics is 1.0121,
indicating relatively similar rates as the reference group (Caucasians),
suggesting fair treatment in this aspect.</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>
<strong>False positive rate parity</strong>: Compares the false
positive rates (e.g., for non-reoffenders) of each ethnic group with the
reference group.</li>
</ol>
<p><span class="math display">\[FPRP = \frac{FP}{TN + FP}\]</span></p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"fprp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: false positive rate parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.8739 [1.7613, 1.988], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##  Asian: 0.39222 [0.048308, 1.2647], p-value = 0.19944</span></span>
<span><span class="co">##  Hispanic: 0.85983 [0.67237, 1.0736], p-value = 0.25424</span></span>
<span><span class="co">##  Native_American: 1.5035 [0.19518, 3.5057], p-value = 0.61986</span></span>
<span><span class="co">##  Other: 0.67967 [0.47835, 0.92493], p-value = 0.019574</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The false positive rate parity ratio for
African Americans is 1.8739. This indicates that African Americans are
approximately 1.87 times more likely to be falsely predicted as
reoffenders than Caucasians. This suggests potential bias in the
algorithm’s false positive predictions in favor of African
Americans.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.39222, indicating that
they are less likely to be falsely predicted as reoffenders compared to
Caucasians. This suggests potential fair treatment of Asians in false
positive predictions.</li>
<li>
<em>Hispanic</em>: The false positive rate parity ratio for
Hispanics is 0.85983, suggesting they are less likely to be falsely
predicted as reoffenders compared to Caucasians. This indicates
potential fair treatment of Hispanics in false positive
predictions.</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>
<strong>True positive rate parity (Equal opportunity)</strong>:
Compares the true positive rates (e.g., for reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<p><span class="math display">\[TPRP = \frac{TP}{TP + FN}\]</span></p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"tprp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: Ttrue positive rate parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 1.5943 [1.5488, 1.638], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##  Asian: 0.52964 [0.067485, 1.3789], p-value = 0.29386</span></span>
<span><span class="co">##  Hispanic: 0.98642 [0.83236, 1.1428], p-value = 0.93562</span></span>
<span><span class="co">##  Native_American: 1.2711 [0.31065, 2.0068], p-value = 0.67157</span></span>
<span><span class="co">##  Other: 0.88843 [0.70202, 1.0832], p-value = 0.28911</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The true positive rate parity ratio for
African Americans is 1.5943. This indicates that African Americans are
approximately 1.59 times more likely to be correctly predicted as
reoffenders than Caucasians. This suggests potential favoritism towards
African Americans in true positive predictions made by the
algorithm.</li>
<li>
<em>Asian</em>: The ratio for Asians is 0.52964, indicating that
they are less likely to be correctly predicted as reoffenders compared
to Caucasians. This suggests potential underestimation of reoffenders
among Asians by the algorithm.</li>
<li>
<em>Hispanic:</em> The true positive rate parity ratio for Hispanics
is 0.98642, suggesting their true positive rate is very close to that of
the reference group (Caucasians). This indicates relatively fair
treatment in the algorithm’s true positive predictions.</li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li>
<strong>Negative predicted value parity</strong>: Compares the
negative predicted values (e.g., for non-reoffenders) of each ethnic
group with the reference group.</li>
</ol>
<p><span class="math display">\[NPVP = \frac{TN}{TN + FN}\]</span></p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"npvp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: negative predictive value parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 0.98013 [0.94264, 1.0164], p-value = 0.45551</span></span>
<span><span class="co">##  Asian: 1.1163 [0.82877, 1.3116], p-value = 0.52546</span></span>
<span><span class="co">##  Hispanic: 1.0326 [0.96161, 1.0984], p-value = 0.43956</span></span>
<span><span class="co">##  Native_American: 0.95687 [0.31975, 1.3732], p-value = 1</span></span>
<span><span class="co">##  Other: 1.0348 [0.95007, 1.112], p-value = 0.46084</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The Negative predicted value (NPV) parity
ratio for African Americans is 0.98013. A value close to 1 indicates
that the NPV for African Americans is very similar to the reference
group (Caucasians). This suggests fair treatment in predicting
non-reoffenders among African Americans.</li>
<li>
<em>Asian</em>: The NPV parity ratio for Asians is 1.1163,
indicating that their NPV is slightly higher than for Caucasians. This
could suggest potential favoritism towards Asians in predicting
non-reoffenders.</li>
<li>
<em>Hispanic</em>: The NPV parity ratio for Hispanics is 1.0326,
suggesting that their NPV is slightly higher than for Caucasians. This
indicates potential favoritism towards Hispanics in predicting
non-reoffenders.</li>
</ul>
<ol start="9" style="list-style-type: decimal">
<li>
<strong>Specificity parity (True negative rate parity)</strong>:
Compares the specificity (true negative rate) of each ethnic group with
the reference group.</li>
</ol>
<p><span class="math display">\[SP = \frac{TN}{TN + FP}\]</span></p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"sp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## metric: specificity parity, reference group: Caucasian, positive class: yes</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Sample estimates (parity ratio):</span></span>
<span><span class="co">##  African_American: 0.75105 [0.71855, 0.78314], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##  Asian: 1.1731 [0.92461, 1.2711], p-value = 0.19944</span></span>
<span><span class="co">##  Hispanic: 1.0399 [0.97904, 1.0933], p-value = 0.25424</span></span>
<span><span class="co">##  Native_American: 0.85657 [0.28624, 1.2293], p-value = 0.61986</span></span>
<span><span class="co">##  Other: 1.0912 [1.0214, 1.1486], p-value = 0.019574</span></span></code></pre>
<ul>
<li>
<em>African American</em>: The specificity parity ratio for African
Americans is 0.75105. A value lower than 1 indicates that the
specificity for African Americans is lower than for Caucasians. This
suggests potential bias in correctly identifying non-reoffenders among
African Americans.</li>
<li>
<em>Asian</em>: The specificity parity ratio for Asians is 1.1731,
indicating their specificity is slightly higher than for Caucasians.
This could suggest potential favoritism in correctly identifying
non-reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The specificity parity ratio for Hispanics is
1.0399, suggesting that their specificity is very close to the reference
group (Caucasians). This indicates relatively fair treatment in
correctly identifying non-reoffenders among Hispanics.</li>
</ul>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li>Büyük, S. (2023). <em>Automatic Fairness Criteria and Fair Model
Selection for Critical ML Tasks</em>, Master Thesis, Utrecht University.
- <a href="https://studenttheses.uu.nl/handle/20.500.12932/44225" class="external-link">View
Online</a>
</li>
<li>Calders, T., &amp; Verwer, S. (2010). Three naive Bayes approaches
for discrimination-free classification. In <em>Data Mining and Knowledge
Discovery</em>. Springer Science and Business Media LLC. - <a href="https://doi.org/10.1007/s10618-010-0190-x" class="external-link">View Online</a>
</li>
<li>Chouldechova, A. (2017). Fair Prediction with Disparate Impact: A
Study of Bias in Recidivism Prediction Instruments. In <em>Big
Data</em>. Mary Ann Liebert Inc. - <a href="https://doi.org/10.1089/big.2016.0047" class="external-link">View Online</a>
</li>
<li>Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp;
Venkatasubramanian, S. (2015). Certifying and Removing Disparate Impact.
In *Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM. - <a href="https://doi.org/10.1145/2783258.2783311" class="external-link">View Online</a>
</li>
<li>Fisher, R. A. (1970). <em>Statistical Methods for Research
Workers</em>. Oliver &amp; Boyd.</li>
<li>Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary,
S., Hamilton, E. P., &amp; Roth, D. (2019). A comparative study of
fairness-enhancing interventions in machine learning. In <em>Proceedings
of the Conference on Fairness, Accountability, and Transparency</em>.
ACM. - <a href="https://doi.org/10.1145/3287560.3287589" class="external-link">View
Online</a>
</li>
<li>Jamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., &amp;
Wagenmakers, E. J. (2017). Default “Gunel and Dickey” Bayes factors for
contingency tables. , 49, 638-652. - <a href="https://doi.org/10.3758/s13428-016-0739-8" class="external-link">View Online</a>
</li>
<li>Pessach, D. &amp; Shmueli, E. (2022). A review on fairness in
machine learning. <em>ACM Computing Surveys</em>, 55(3), 1-44. - <a href="https://doi.org/10.1145/3494672" class="external-link">View Online</a>
</li>
<li>Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P.
(2017). Fairness Beyond Disparate Treatment &amp; Disparate Impact. In
<em>Proceedings of the 26th International Conference on World Wide
Web</em>. - <a href="https://doi.org/10.1145/3038912.3052660" class="external-link">View
Online</a>
</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://koenderks.com" class="external-link">Koen Derks</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
