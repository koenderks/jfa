<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Algorithmic fairness • jfa</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Algorithmic fairness">
<meta property="og:description" content="jfa">
<meta property="og:image" content="https://koenderks.github.io/jfa/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">jfa</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.7.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/jfa.html">Get started</a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/koenderks/jfa" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/koenderks/jfa/discussions" class="external-link">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Algorithmic fairness</h1>
                        <h4 data-toc-skip class="author">Koen Derks</h4>
            
            <h4 data-toc-skip class="date">2023-11-25</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/koenderks/jfa/blob/HEAD/vignettes/model-fairness.Rmd" class="external-link"><code>vignettes/model-fairness.Rmd</code></a></small>
      <div class="hidden name"><code>model-fairness.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Welcome to the ‘Algorithmic fairness’ vignette of the
<strong>jfa</strong> package. In this vignette you can find a detailed
example of how you can use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function
provided by the package.</p>
</div>
<div class="section level2">
<h2 id="function-model_fairness">Function: <code>model_fairness()</code><a class="anchor" aria-label="anchor" href="#function-model_fairness"></a>
</h2>
<p>The <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function provides a method to
assess fairness in algorithmic decision-making systems by computing
various model-agnostic metrics based on the observed and predicted
labels in a data set. Calculated metrics include demographic parity,
proportional parity, predictive rate parity, accuracy parity, false
negative rate parity, false positive rate parity, true positive rate
parity, negative predicted value parity, and specificity parity <span class="citation">(Calders &amp; Verwer, 2010; Chouldechova, 2017;
Feldman et al., 2015; Friedler et al., 2019; Zafar et al.,
2017)</span>.</p>
<p><em>Example:</em></p>
<p>To illustrate how to use the <code><a href="../reference/model_fairness.html">model_fairness()</a></code> function,
we will use a well-known data set called COMPAS. The COMPAS
(Correctional Offender Management Profiling for Alternative Sanctions)
software is is a case management and decision support tool used by some
U.S. courts to assess the likelihood of a defendant becoming a
recidivist (repeated offender).</p>
<p>The <code>compas</code> data is included in the package and contains
predictions of the COMPAS software for several cases. The data can be
loaded with <code>data("compas")</code> and contains for each defendant,
whether the defendant did commit a crime within two years after the
court case (<code>TwoYrRecidivism</code>), some personal characteristics
like gender and ethnicity, and whether the software predicted the
defendant to be a recidivist (<code>Predicted</code>).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"compas"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">compas</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    TwoYrRecidivism AgeAboveFoutryFive AgeBelowTwentyFive Gender Misdemeanor</span></span>
<span><span class="co">## 4               no                 no                 no   Male         yes</span></span>
<span><span class="co">## 5              yes                 no                 no   Male          no</span></span>
<span><span class="co">## 7               no                 no                 no Female         yes</span></span>
<span><span class="co">## 11              no                 no                 no   Male          no</span></span>
<span><span class="co">## 14              no                 no                 no   Male         yes</span></span>
<span><span class="co">## 24              no                 no                 no   Male         yes</span></span>
<span><span class="co">##           Ethnicity Predicted</span></span>
<span><span class="co">## 4             Other        no</span></span>
<span><span class="co">## 5         Caucasian       yes</span></span>
<span><span class="co">## 7         Caucasian        no</span></span>
<span><span class="co">## 11 African_American        no</span></span>
<span><span class="co">## 14         Hispanic        no</span></span>
<span><span class="co">## 24            Other        no</span></span></code></pre>
<p>We will investigate whether the algorithm is fair with respect to the
sensitive attribute <code>Ethnicity</code>. Considering the context, a
positive prediction means that a defendant is classified as a
reoffender, and a negative prediction means that a defendant is
classified as a non-reoffender. The fairness metrics offer information
on whether there are any disparities in the algorithm’s predictions
across different ethnic groups. By calculating and reviewing these
metrics, we can get an indication of whether the algorithm exhibits any
discriminatory behavior towards specific ethnic groups. If substantial
disparities exist, we may need to investigate further and potentially
modify the algorithm to ensure fairness in its predictions.</p>
<p>Before starting, let’s briefly explain the foundation of all fairness
metrics: the confusion matrix. This matrix presents observed versus
predicted labels, shedding light on the algorithm’s prediction mistakes.
Comprising the confusion matrix are the true positives (TP), false
positives (FP), true negatives (TN), and false negatives (FN). To
illustrate, the confusion matrix pertaining to the
<code>African_American</code> group is displayed below. For example,
there are 629 people in this group that are incorrectly predicted to be
a reoffender, which represents a false positive in the confusion
matrix.</p>
<table class="table">
<colgroup>
<col width="39%">
<col width="28%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th align="right"></th>
<th align="center">
<code>Predicted</code> = <code>no</code>
</th>
<th align="center">
<code>Predicted</code> = <code>yes</code>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">
<code>TwoYrRecidivism</code> = <code>no</code>
</td>
<td align="center">885 (<code>TN</code>)</td>
<td align="center">629 (<code>FP</code>)</td>
</tr>
<tr class="even">
<td align="right">
<code>TwoYrRecidivism</code> = <code>yes</code>
</td>
<td align="center">411 (<code>FN</code>)</td>
<td align="center">1250 (<code>TP</code>)</td>
</tr>
</tbody>
</table>
<p>For illustrative purposes, let’s interpret the full set of fairness
metrics for the African American, Asian, and Hispanic groups in
comparison to the privileged group (Caucasian). See <span class="citation">Pessach &amp; Shmueli (2022)</span> for a more detailed
explanation of some of these metrics. However, note that not all
fairness measures are equally appropriate in all audit situations. The
decision tree below aids the auditor with choosing which measure is best
for the situation at hand <span class="citation">(Büyük,
2023)</span>.</p>
<p align="center">
<img src="fairness-tree.png" alt="fairness" width="90%"></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Demographic parity (Statistical parity)</strong>:
Compares the number of positive predictions (i.e., reoffenders) between
each unprivileged (i.e., ethnic) group and the privileged group. Note
that, since demographic parity is not a proportion, statistical
inference about its equality to the privileged group is not
supported.</p>
<p>The formula for the number of positive predictions is <span class="math inline">\(P = TP + FP\)</span>, and the demographic parity
for unprivileged group <span class="math inline">\(i\)</span> is given
by <span class="math inline">\(DP =
\frac{P_{i}}{P_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"dp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 2.7961</span></span>
<span><span class="co">##   Asian: 0.0059524</span></span>
<span><span class="co">##   Hispanic: 0.22173</span></span>
<span><span class="co">##   Native_American: 0.0074405</span></span>
<span><span class="co">##   Other: 0.12649</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The demographic parity for African
Americans compared to Caucasians is 2.7961, indicating that for these
data there are nearly three times more African Americans predicted as
reoffenders by the algorithm than Caucasians.</li>
<li>
<em>Asian</em>: The demographic parity for Asians is very close to
zero (0.0059524), indicating that there are many less Asians (4) that
are predicted as reoffenders in these data than there are Caucasians
(672). Naturally, this can be explained because of the lack of Asian
people (31) in the data.</li>
<li>
<em>Hispanic</em>: The demographic parity for Hispanics is 0.22173,
meaning that there are about five times less Hispanics predicted as
reoffenders in these data than that there are Caucasians.</li>
</ul>
</li>
<li>
<p><strong>Proportional parity (Disparate impact)</strong>: Compares
the proportion of positive predictions of each unprivileged group to
that in the privileged group. For example, in the case that a positive
prediction represents a reoffender, proportional parity requires the
proportion of predicted reoffenders to be similar across ethnic
groups.</p>
<p>The formula for the proportion of positive predictions is <span class="math inline">\(PP = \frac{TP + FP}{TP + FP + TN + FN}\)</span>,
and the proportional parity for unprivileged group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(\frac{PP_{i}}{PP_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 522.28, df = 5, p-value &lt; 2.2e-16</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.8521 [1.7978, 1.9058], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian: 0.4038 [0.1136, 0.93363], p-value = 0.030318</span></span>
<span><span class="co">##   Hispanic: 0.91609 [0.79339, 1.0464], p-value = 0.26386</span></span>
<span><span class="co">##   Native_American: 1.4225 [0.52415, 2.3978], p-value = 0.3444</span></span>
<span><span class="co">##   Other: 0.77552 [0.63533, 0.92953], p-value = 0.0080834</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The proportional parity for African
Americans compared to Caucasians is 1.8521. This indicates that African
Americans are approximately 1.85 times more likely to get a positive
prediction than Caucasians. Again, this suggests potential bias in the
algorithm’s predictions against African Americans. The p-value is
smaller than .05, indicating that the null hypothesis of proportional
parity should be rejected <span class="citation">(Fisher,
1970)</span>.</li>
<li>
<em>Asian</em>: The proportional parity for Asians is 0.4038,
indicating that their positive prediction rate is lower than for
Caucasians. This may suggest potential underestimation of reoffenders
among Asians.</li>
<li>
<em>Hispanic</em>: The proportional parity for Hispanics is 0.91609,
suggesting that their positive prediction rate is close to the
privileged group. This indicates relatively fair treatment of Hispanics
in the algorithm’s predictions.</li>
</ul>
<p>This is a good time to show the <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> functions associated with the
<code><a href="../reference/model_fairness.html">model_fairness()</a></code> function. Let’s examine the previous
function call again, but instead of printing the output to the console,
this time we store the output in <code>x</code> and run the
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> functions on this
object.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test Summary</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Options:</span></span>
<span><span class="co">##   Confidence level:    0.95 </span></span>
<span><span class="co">##   Fairness metric:     Proportional parity (Disparate impact)</span></span>
<span><span class="co">##   Model type:          Binary classification</span></span>
<span><span class="co">##   Privileged group:    Caucasian</span></span>
<span><span class="co">##   Positive class:      yes </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Data:</span></span>
<span><span class="co">##   Sample size:         6172 </span></span>
<span><span class="co">##   Unprivileged groups: 5 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Results:</span></span>
<span><span class="co">##   X-squared:           522.28 </span></span>
<span><span class="co">##   Degrees of freedom:  5 </span></span>
<span><span class="co">##   p-value:             &lt; 2.22e-16 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Comparisons to privileged (P) group:</span></span>
<span><span class="co">##                                     Proportion                     Parity</span></span>
<span><span class="co">##   Caucasian (P)     0.31954 [0.29964, 0.33995]                          -</span></span>
<span><span class="co">##   African_American  0.59181 [0.57448, 0.60897]    1.8521 [1.7978, 1.9058]</span></span>
<span><span class="co">##   Asian            0.12903 [0.036302, 0.29834]   0.4038 [0.1136, 0.93363]</span></span>
<span><span class="co">##   Hispanic          0.29273 [0.25352, 0.33437]  0.91609 [0.79339, 1.0464]</span></span>
<span><span class="co">##   Native_American   0.45455 [0.16749, 0.76621]   1.4225 [0.52415, 2.3978]</span></span>
<span><span class="co">##   Other             0.24781 [0.20302, 0.29702] 0.77552 [0.63533, 0.92953]</span></span>
<span><span class="co">##                                     Odds ratio    p-value</span></span>
<span><span class="co">##   Caucasian (P)                              -          -</span></span>
<span><span class="co">##   African_American     3.0866 [2.7453, 3.4726] &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian            0.31561 [0.079942, 0.91095]   0.030318</span></span>
<span><span class="co">##   Hispanic           0.88138 [0.70792, 1.0939]    0.26386</span></span>
<span><span class="co">##   Native_American     1.7739 [0.42669, 7.0041]     0.3444</span></span>
<span><span class="co">##   Other             0.70166 [0.53338, 0.91633]  0.0080834</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model performance:</span></span>
<span><span class="co">##                    Support  Accuracy Precision    Recall  F1 score</span></span>
<span><span class="co">##   Caucasian           2103 0.6585830 0.5773810 0.4720195 0.5194110</span></span>
<span><span class="co">##   African_American    3175 0.6724409 0.6652475 0.7525587 0.7062147</span></span>
<span><span class="co">##   Asian                 31 0.7419355 0.5000000 0.2500000 0.3333333</span></span>
<span><span class="co">##   Hispanic             509 0.6817289 0.5906040 0.4656085 0.5207101</span></span>
<span><span class="co">##   Native_American       11 0.6363636 0.6000000 0.6000000 0.6000000</span></span>
<span><span class="co">##   Other                343 0.6938776 0.6117647 0.4193548 0.4976077</span></span></code></pre>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">x</span>, type <span class="op">=</span> <span class="st">"estimates"</span><span class="op">)</span></span></code></pre></div>
<p><img src="model-fairness_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;"></p>
</li>
<li>
<p><strong>Predictive rate parity (Equalized odds)</strong>:
Compares the overall positive prediction rates (e.g., the precision) of
each unprivileged group to the privileged group.</p>
<p>The formula for the precision is <span class="math inline">\(PR =
\frac{TP}{TP + FP}\)</span>, and the predictive rate parity for
unprivileged group <span class="math inline">\(i\)</span> is given by
<span class="math inline">\(PRP =
\frac{PR_{i}}{PR_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"prp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 18.799, df = 5, p-value = 0.002095</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.1522 [1.1143, 1.1891], p-value = 5.4523e-05</span></span>
<span><span class="co">##   Asian: 0.86598 [0.11706, 1.6149], p-value = 1</span></span>
<span><span class="co">##   Hispanic: 1.0229 [0.87836, 1.1611], p-value = 0.78393</span></span>
<span><span class="co">##   Native_American: 1.0392 [0.25396, 1.6406], p-value = 1</span></span>
<span><span class="co">##   Other: 1.0596 [0.86578, 1.2394], p-value = 0.5621</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The predictive rate parity for African
Americans is 1.1522. This suggests that the precision for African
Americans is approximately 1.15 times higher than for Caucasians, which
indicates potential ‘favoritism’ towards African Americans in the
overall positive predictions made by the algorithm.</li>
<li>
<em>Asian</em>: The predictive rate parity for Asians is 0.86598,
indicating that their precision is lower than for Caucasians. This
suggests potential underestimation of reoffenders among Asians by the
algorithm.</li>
<li>
<em>Hispanic</em>: The predictive rate parity for Hispanics is
1.0229, suggesting their overall positive prediction rate is very close
to that of the privileged group (Caucasians). This indicates relatively
fair treatment in the algorithm’s overall positive predictions.</li>
</ul>
</li>
<li>
<p><strong>Accuracy parity</strong>: Compares the accuracy of each
unprivileged group’s predictions with the privileged group.</p>
<p>The formula for the accuracy is <span class="math inline">\(A =
\frac{TP + TN}{TP + FP + TN + FN}\)</span>, and the accuracy parity for
unprivileged group <span class="math inline">\(i\)</span> is given by
<span class="math inline">\(AP =
\frac{A_{i}}{A_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"ap"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 3.3081, df = 5, p-value = 0.6526</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.021 [0.99578, 1.0458], p-value = 0.29669</span></span>
<span><span class="co">##   Asian: 1.1266 [0.841, 1.3384], p-value = 0.44521</span></span>
<span><span class="co">##   Hispanic: 1.0351 [0.97074, 1.0963], p-value = 0.34691</span></span>
<span><span class="co">##   Native_American: 0.96626 [0.46753, 1.3525], p-value = 1</span></span>
<span><span class="co">##   Other: 1.0536 [0.975, 1.127], p-value = 0.21778</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>African American: The accuracy parity for African Americans is
1.021, suggesting their accuracy is very similar to the privileged group
(Caucasians). This indicates fair treatment concerning overall
accuracy.</li>
<li>
<em>Asian</em>: The accuracy parity for Asians is 1.1266, suggesting
their accuracy is slightly higher than for Caucasians, indicating
potential favoritism in overall accuracy.</li>
<li>
<em>Hispanic</em>: The accuraty parity for Hispanics is 1.0351,
suggesting their accuracy is slightly higher than for Caucasians,
indicating potential favoritism in overall accuracy.</li>
</ul>
</li>
<li>
<p><strong>False negative rate parity (Treatment equality)</strong>:
Compares the false negative rates of each unprivileged group with the
privileged group.</p>
<p>The formula for the false negative rate is <span class="math inline">\(FNR = \frac{FN}{TP + FN}\)</span>, and the false
negative rate parity for unprivileged group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(FNRP =
\frac{FNR_{i}}{FNR_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"fnrp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 246.59, df = 5, p-value &lt; 2.2e-16</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 0.46866 [0.42965, 0.50936], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian: 1.4205 [0.66128, 1.8337], p-value = 0.29386</span></span>
<span><span class="co">##   Hispanic: 1.0121 [0.87234, 1.1499], p-value = 0.93562</span></span>
<span><span class="co">##   Native_American: 0.7576 [0.099899, 1.6163], p-value = 0.67157</span></span>
<span><span class="co">##   Other: 1.0997 [0.92563, 1.2664], p-value = 0.28911</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The parity of false negative rates (FNRs)
for African Americans compared to Caucasians is 0.46866. A value lower
than 1 suggests that African Americans are less likely to be falsely
classified as non-reoffenders, indicating potential bias against this
group in this aspect.</li>
<li>
<em>Asian</em>: The parity for Asians is 1.4205, indicating that
they are more likely to be falsely classified as non-reoffenders
compared to Caucasians, suggesting potential underestimation of
reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The FNR parity for Hispanics is 1.0121,
indicating relatively similar rates as the privileged group
(Caucasians), suggesting fair treatment in this aspect.</li>
</ul>
</li>
<li>
<p><strong>False positive rate parity</strong>: Compares the false
positive rates (e.g., for non-reoffenders) of each unprivileged group
with the privileged group.</p>
<p>The formula for the false positive rate is <span class="math inline">\(FPR = \frac{FP}{TN + FP}\)</span>, and the false
positive rate parity for unprivileged group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(FPRP =
\frac{FPR_{i}}{FPR_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"fprp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 179.76, df = 5, p-value &lt; 2.2e-16</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.8739 [1.7613, 1.988], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian: 0.39222 [0.048308, 1.2647], p-value = 0.19944</span></span>
<span><span class="co">##   Hispanic: 0.85983 [0.67237, 1.0736], p-value = 0.25424</span></span>
<span><span class="co">##   Native_American: 1.5035 [0.19518, 3.5057], p-value = 0.61986</span></span>
<span><span class="co">##   Other: 0.67967 [0.47835, 0.92493], p-value = 0.019574</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The false positive rate parity for
African Americans is 1.8739. This indicates that African Americans are
approximately 1.87 times more likely to be falsely predicted as
reoffenders than Caucasians. This suggests potential bias in the
algorithm’s false positive predictions in favor of African
Americans.</li>
<li>
<em>Asian</em>: The parity for Asians is 0.39222, indicating that
they are less likely to be falsely predicted as reoffenders compared to
Caucasians. This suggests potential fair treatment of Asians in false
positive predictions.</li>
<li>
<em>Hispanic</em>: The false positive rate parity for Hispanics is
0.85983, suggesting they are less likely to be falsely predicted as
reoffenders compared to Caucasians. This indicates potential fair
treatment of Hispanics in false positive predictions.</li>
</ul>
</li>
<li>
<p><strong>True positive rate parity (Equal opportunity)</strong>:
Compares the true positive rates (e.g., for reoffenders) of each
unprivileged group with the privileged group.</p>
<p>The formula for the true positive rate is <span class="math inline">\(TPR = \frac{TP}{TP + FN}\)</span>, and the true
positive rate parity for unprivileged group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(TPRP =
\frac{TPR_{i}}{TPR_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"tprp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 246.59, df = 5, p-value &lt; 2.2e-16</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.5943 [1.5488, 1.638], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian: 0.52964 [0.067485, 1.3789], p-value = 0.29386</span></span>
<span><span class="co">##   Hispanic: 0.98642 [0.83236, 1.1428], p-value = 0.93562</span></span>
<span><span class="co">##   Native_American: 1.2711 [0.31065, 2.0068], p-value = 0.67157</span></span>
<span><span class="co">##   Other: 0.88843 [0.70202, 1.0832], p-value = 0.28911</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The true positive rate parity for African
Americans is 1.5943. This indicates that African Americans are
approximately 1.59 times more likely to be correctly predicted as
reoffenders than Caucasians. This suggests potential favoritism towards
African Americans in true positive predictions made by the
algorithm.</li>
<li>
<em>Asian</em>: The parity for Asians is 0.52964, indicating that
they are less likely to be correctly predicted as reoffenders compared
to Caucasians. This suggests potential underestimation of reoffenders
among Asians by the algorithm.</li>
<li>
<em>Hispanic:</em> The true positive rate parity for Hispanics is
0.98642, suggesting their true positive rate is very close to that of
the privileged group (Caucasians). This indicates relatively fair
treatment in the algorithm’s true positive predictions.</li>
</ul>
</li>
<li>
<p><strong>Negative predicted value parity</strong>: Compares the
negative predicted value (e.g., for non-reoffenders) of each
unprivileged group with that of the privileged group.</p>
<p>The formula for the negative predicted value is <span class="math inline">\(NPV = \frac{TN}{TN + FN}\)</span>, and the
negative predicted value parity for unprivileged group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(NPVP =
\frac{NPV_{i}}{NPV_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"npvp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 3.6309, df = 5, p-value = 0.6037</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 0.98013 [0.94264, 1.0164], p-value = 0.45551</span></span>
<span><span class="co">##   Asian: 1.1163 [0.82877, 1.3116], p-value = 0.52546</span></span>
<span><span class="co">##   Hispanic: 1.0326 [0.96161, 1.0984], p-value = 0.43956</span></span>
<span><span class="co">##   Native_American: 0.95687 [0.31975, 1.3732], p-value = 1</span></span>
<span><span class="co">##   Other: 1.0348 [0.95007, 1.112], p-value = 0.46084</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The negative predicted value parity for
African Americans is 0.98013. A value close to 1 indicates that the
negative predicted value for African Americans is very similar to the
privileged group (Caucasians). This suggests fair treatment in
predicting non-reoffenders among African Americans.</li>
<li>
<em>Asian</em>: The negative predicted value parity for Asians is
1.1163, indicating that their negative predicted value is slightly
higher than for Caucasians. This could suggest potential favoritism
towards Asians in predicting non-reoffenders.</li>
<li>
<em>Hispanic</em>: The negative predicted value parity for Hispanics
is 1.0326, suggesting that their negative predicted value is slightly
higher than for Caucasians. This indicates potential favoritism towards
Hispanics in predicting non-reoffenders.</li>
</ul>
</li>
<li>
<p><strong>Specificity parity (True negative rate parity)</strong>:
Compares the specificity (true negative rate) of each unprivileged group
with the privileged group.</p>
<p>The formula for the specificity is <span class="math inline">\(S =
\frac{TN}{TN + FP}\)</span>, and the specificity parity for unprivileged
group <span class="math inline">\(i\)</span> is given by <span class="math inline">\(SP = \frac{S_{i}}{S_{privileged}}\)</span>.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"sp"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Classical Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, X-squared = 179.76, df = 5, p-value &lt; 2.2e-16</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 0.75105 [0.71855, 0.78314], p-value = &lt; 2.22e-16</span></span>
<span><span class="co">##   Asian: 1.1731 [0.92461, 1.2711], p-value = 0.19944</span></span>
<span><span class="co">##   Hispanic: 1.0399 [0.97904, 1.0933], p-value = 0.25424</span></span>
<span><span class="co">##   Native_American: 0.85657 [0.28624, 1.2293], p-value = 0.61986</span></span>
<span><span class="co">##   Other: 1.0912 [1.0214, 1.1486], p-value = 0.019574</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p><strong><em>Interpretation:</em></strong></p>
<ul>
<li>
<em>African American</em>: The specificity parity for African
Americans is 0.75105. A value lower than 1 indicates that the
specificity for African Americans is lower than for Caucasians. This
suggests potential bias in correctly identifying non-reoffenders among
African Americans.</li>
<li>
<em>Asian</em>: The specificity parity for Asians is 1.1731,
indicating their specificity is slightly higher than for Caucasians.
This could suggest potential favoritism in correctly identifying
non-reoffenders among Asians.</li>
<li>
<em>Hispanic</em>: The specificity parity for Hispanics is 1.0399,
suggesting that their specificity is very close to the privileged group.
This indicates relatively fair treatment in correctly identifying
non-reoffenders among Hispanics.</li>
</ul>
</li>
</ol>
<div class="section level3">
<h3 id="bayesian-analysis">Bayesian Analysis<a class="anchor" aria-label="anchor" href="#bayesian-analysis"></a>
</h3>
<p>For all metrics except demongraphic parity, Bayesian inference is
supported and provides credible intervals and Bayes factors <span class="citation">(Jamil et al., 2017)</span>. Like in the other
functions in <code>jfa</code>, performing a Bayesian analysis using a
default prior can be achieved by setting <code>prior = TRUE</code>.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_fairness.html">model_fairness</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">compas</span>,</span>
<span>  protected <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span>
<span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span>
<span>  privileged <span class="op">=</span> <span class="st">"Caucasian"</span>,</span>
<span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"pp"</span>,</span>
<span>  prior <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Bayesian Algorithmic Fairness Test</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data: compas</span></span>
<span><span class="co">## n = 6172, BF₁₀ = 3.497e+110</span></span>
<span><span class="co">## alternative hypothesis: fairness metrics are not equal across groups</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##   African_American: 1.8473 [1.7943, 1.9008], BF₁₀ = 2.8881e+81</span></span>
<span><span class="co">##   Asian: 0.34791 [0.17218, 0.91485], BF₁₀ = 3.0278</span></span>
<span><span class="co">##   Hispanic: 0.91193 [0.80059, 1.0403], BF₁₀ = 0.1127</span></span>
<span><span class="co">##   Native_American: 0.89975 [0.65344, 2.2292], BF₁₀ = 0.54521</span></span>
<span><span class="co">##   Other: 0.78076 [0.64145, 0.92384], BF₁₀ = 2.5057</span></span>
<span><span class="co">## alternative hypothesis: true odds ratio is not equal to 1</span></span></code></pre>
<p>Bayesian inference measures evidence using the Bayes factor <span class="math inline">\(BF_{01}\)</span>, which quantifies the evidence in
favor of algorithmic fairness (i.e., equal fairness metrics across all
groups) over algorithmic bias or <span class="math inline">\(BF_{10}\)</span>, which quantifies the evidence in
favor of algorithmic bias over algorithmic fairness. By default,
<strong>jfa</strong> reports <span class="math inline">\(BF_{10}\)</span>, but <span class="math inline">\(BF_{01} = \frac{1}{BF_{10}}\)</span>. The
resulting Bayes factor in favor of rejection of the null hypothesis of
equality (<span class="math inline">\(BF_{10}\)</span>) is shown in the
output above. As can be seen, <span class="math inline">\(BF_{10}\)</span> &gt; 1000, which indicates
extreme evidence in favor of bias between the groups.</p>
<p>In a Bayesian analysis, the auditor specifies a prior distribution
that can incorporate relevant audit information. The prior distribution
for this analysis is expressed on the log odds ratio and can be adjusted
by setting <code>prior = 1</code> (the default), or providing a number
&gt; 1 representing the prior concentration parameter. The larger the
concentration parameter, the more the prior distribution is concentrated
around zero, meaning equal metrics are assumed to be more likely. The
prior and posterior distributions on the log odds ratio can be
visualized via the <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> function using
<code>type = "posterior"</code>, see the figure below.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">x</span>, type <span class="op">=</span> <span class="st">"posterior"</span><span class="op">)</span></span></code></pre></div>
<p><img src="model-fairness_files/figure-html/unnamed-chunk-14-1.png" width="576" style="display: block; margin: auto;"></p>
<p>You can check the robustness of the Bayes factor to the choice of
prior distribution via the <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> function using
<code>type = "robustness"</code>, see the figure below.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">x</span>, type <span class="op">=</span> <span class="st">"robustness"</span><span class="op">)</span></span></code></pre></div>
<p><img src="model-fairness_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;"></p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-buyuk_2023" class="csl-entry">
Büyük, S. (2023). <em>Automatic fairness criteria and fair model
selection for critical ML tasks</em> [Master’s thesis, Utrecht
University]. <a href="https://studenttheses.uu.nl/handle/20.500.12932/44225" class="external-link">https://studenttheses.uu.nl/handle/20.500.12932/44225</a>
</div>
<div id="ref-calders_2010" class="csl-entry">
Calders, T., &amp; Verwer, S. (2010). Three naive bayes approaches for
discrimination-free classification. <em>Data Mining and Knowledge
Discovery</em>. <a href="https://doi.org/10.1007/s10618-010-0190-x" class="external-link">https://doi.org/10.1007/s10618-010-0190-x</a>
</div>
<div id="ref-chouldechova_2017" class="csl-entry">
Chouldechova, A. (2017). Fair prediction with disparate impact: A study
of bias in recidivism prediction instruments. <em>Big Data</em>. <a href="https://doi.org/10.1089/big.2016.0047" class="external-link">https://doi.org/10.1089/big.2016.0047</a>
</div>
<div id="ref-feldman_2015" class="csl-entry">
Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp;
Venkatasubramanian, S. (2015). Certifying and removing disparate impact.
<em>Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining</em>. <a href="https://doi.org/10.1145/2783258.2783311" class="external-link">https://doi.org/10.1145/2783258.2783311</a>
</div>
<div id="ref-fisher_1970" class="csl-entry">
Fisher, R. A. (1970). <em>Statistical methods for research workers</em>.
Oliver &amp; Boyd.
</div>
<div id="ref-friedler_2019" class="csl-entry">
Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S.,
Hamilton, E. P., &amp; Roth, D. (2019). A comparative study of
fairness-enhancing interventions in machine learning. <em>Proceedings of
the Conference on Fairness, Accountability, and Transparency</em>. <a href="https://doi.org/10.1145/3287560.3287589" class="external-link">https://doi.org/10.1145/3287560.3287589</a>
</div>
<div id="ref-jamil_2017" class="csl-entry">
Jamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., &amp;
Wagenmakers, E.-J. (2017). Default "gunel and dickey" bayes factors for
contingency tables. <em>Behavior Research Methods</em>, <em>49</em>,
638–652. <a href="https://doi.org/10.3758/s13428-016-0739-8" class="external-link">https://doi.org/10.3758/s13428-016-0739-8</a>
</div>
<div id="ref-pessach_2022" class="csl-entry">
Pessach, D., &amp; Shmueli, E. (2022). A review on fairness in machine
learning. <em>ACM Computing Surveys</em>, <em>55</em>(3), 1–44. <a href="https://doi.org/10.1145/3494672" class="external-link">https://doi.org/10.1145/3494672</a>
</div>
<div id="ref-zafar_2017" class="csl-entry">
Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P.
(2017). Fairness beyond disparate treatment &amp; disparate impact.
<em>Proceedings of the 26th International Conference on World Wide
Web</em>. <a href="https://doi.org/10.1145/3038912.3052660" class="external-link">https://doi.org/10.1145/3038912.3052660</a>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://koenderks.com" class="external-link">Koen Derks</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
