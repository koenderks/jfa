<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Algorithm Auditing: Fairness Metrics and Bias Detection — model_fairness • jfa</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Algorithm Auditing: Fairness Metrics and Bias Detection — model_fairness"><meta property="og:description" content="This function aims to assess fairness and bias in algorithmic
decision-making systems by computing and testing the equality in one of
several model-agnostic fairness metrics. These metrics aim to quantify
fairness across sensitive groups in the data based on the predictions of the
algorithm. The metrics that can be calculated include predictive rate parity,
proportional parity, accuracy parity, false negative rate parity, false
positive rate parity, true positive rate parity, negative predicted value
parity, specificity parity, and demographic parity. Currently, this function
only supports binary classification. The function returns an object of class
jfaModelBias that can be used with associated summary() and
plot() methods."><meta property="og:image" content="https://koenderks.github.io/jfa/logo.png"><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">jfa</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.7.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/jfa.html">Get started</a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/koenderks/jfa" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/koenderks/jfa/discussions" class="external-link">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Algorithm Auditing: Fairness Metrics and Bias Detection</h1>
    <small class="dont-index">Source: <a href="https://github.com/koenderks/jfa/blob/HEAD/R/model_fairness.R" class="external-link"><code>R/model_fairness.R</code></a></small>
    <div class="hidden name"><code>model_fairness.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>This function aims to assess fairness and bias in algorithmic
decision-making systems by computing and testing the equality in one of
several model-agnostic fairness metrics. These metrics aim to quantify
fairness across sensitive groups in the data based on the predictions of the
algorithm. The metrics that can be calculated include predictive rate parity,
proportional parity, accuracy parity, false negative rate parity, false
positive rate parity, true positive rate parity, negative predicted value
parity, specificity parity, and demographic parity. Currently, this function
only supports binary classification. The function returns an object of class
<code>jfaModelBias</code> that can be used with associated <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> methods.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">model_fairness</span><span class="op">(</span></span>
<span>  <span class="va">data</span>,</span>
<span>  <span class="va">sensitive</span>,</span>
<span>  <span class="va">target</span>,</span>
<span>  <span class="va">predictions</span>,</span>
<span>  reference <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  positive <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"prp"</span>, <span class="st">"pp"</span>, <span class="st">"ap"</span>, <span class="st">"fnrp"</span>, <span class="st">"fprp"</span>,</span>
<span>    <span class="st">"tprp"</span>, <span class="st">"npvp"</span>, <span class="st">"sp"</span>, <span class="st">"dp"</span></span>
<span>  <span class="op">)</span>,</span>
<span>  alternative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"two.sided"</span>, <span class="st">"greater"</span>, <span class="st">"less"</span><span class="op">)</span>,</span>
<span>  conf.level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>  prior <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>data</dt>
<dd><p>a data frame containing the input data.</p></dd>


<dt>sensitive</dt>
<dd><p>a character specifying the column name in <code>data</code>
indicating the sensitive variable.</p></dd>


<dt>target</dt>
<dd><p>A character specifying the column name in <code>data</code>
indicating the actual values of the target (to be predicted) variable.</p></dd>


<dt>predictions</dt>
<dd><p>a character specifying the column name in <code>data</code>
indicating the predicted values of the target variable.</p></dd>


<dt>reference</dt>
<dd><p>a character specifying the class in the column
<code>sensitive</code> used as the reference class for computing the fairness
metrics. If <code>NULL</code> (the default), the first factor level of the
<code>sensitive</code> column is used as the reference group.</p></dd>


<dt>positive</dt>
<dd><p>a character specifying the positive class in the column
<code>target</code> used for computing the fairness metrics. If <code>NULL</code> (the
default), the first factor level of the <code>target</code> column is used as the
positive class.</p></dd>


<dt>metric</dt>
<dd><p>a character (vector) indicating the fairness metrics(s)
to compute. See the Details section for more information.</p></dd>


<dt>alternative</dt>
<dd><p>the type of confidence interval to produce. Possible
options are <code>two.sided</code> (the default), <code>greater</code> and <code>less</code>.</p></dd>


<dt>conf.level</dt>
<dd><p>a numeric value between 0 and 1 specifying the
confidence level (i.e., 1 - audit risk / detection risk).</p></dd>


<dt>prior</dt>
<dd><p>a logical specifying whether to use a prior distribution,
or a numeric value equal to or larger than 1 specifying the prior
concentration parameter. If this argument is specified as <code>FALSE</code>
(default), classical estimation is performed and if it is <code>TRUE</code>,
Bayesian estimation using a default prior with concentration parameter 1 is
performed.</p></dd>

</dl></div>
    <div id="value">
    <h2>Value</h2>
    

<p>An object of class <code>jfaModelFairness</code> containing:</p>
<dl><dt>reference</dt>
<dd><p>The reference group for computing the fairness metrics.</p></dd>

<dt>positive</dt>
<dd><p>The positive class used in computing the fairness metrics.</p></dd>

<dt>alternative</dt>
<dd><p>The type of confidence interval.</p></dd>

<dt>confusion.matrix</dt>
<dd><p>A list of confusion matrices for each group.</p></dd>

<dt>performance</dt>
<dd><p>A data frame containing performance metrics for each
  group, including accuracy, precision, recall, and F1 score.</p></dd>

<dt>metric</dt>
<dd><p>A data frame containing, for each group, the estimates of the
  fairness metric along with the associated confidence / credible interval.</p></dd>

<dt>parity</dt>
<dd><p>A data frame containing, for each sensitive group, the parity
  ratio and associated confidence / credible interval when compared to the
  reference group.</p></dd>

<dt>odds.ratio</dt>
<dd><p>A data frame containing, for each sensitive group, the odds
  ratio of the fairness meatrics and associated confidence/credible interval,
  along with any inferential measures, for the comparison to the reference
  group.</p></dd>

<dt>measure</dt>
<dd><p>The abbreviation of the selected fairness metric.</p></dd>

<dt>data.name</dt>
<dd><p>The name of the input data object.</p></dd>

</dl></div>
    <div id="details">
    <h2>Details</h2>
    <p>The following model-agnostic fairness metrics are computed based on
  the confusion matrix for each sensitive group, using the true positives
  (TP), false positives (FP), true negative (TN) and false negatives (FN).
  See Pessach &amp; Shmueli (2022) for a more detailed explanation of the
  individual metrics. The equality of metrics across groups is done according
  to the methodology described in Fisher (1970) and Jamil et al. (2017).</p>
<p></p><ul><li><p>Predictive rate parity (<code>prp</code>): calculated as TP / (TP +
      FP), quantifies whether the positive prediction rate is the same across
      groups.</p></li>
<li><p>Proportional parity (<code>pp</code>): calculated as (TP + FP) / (TP +
      FP + TN + FN), quantifies whether the positive prediction rate is equal
      across groups.</p></li>
<li><p>Accuracy parity (<code>ap</code>): calculated as (TP + TN) / (TP + FP +
      TN + FN), quantifies whether the accuracy is the same across groups.</p></li>
<li><p>False negative rate parity (<code>fnrp</code>): calculated as FN / (FP
      + FN), quantifies whether the false negative rate is the same across
      groups.</p></li>
<li><p>False positive rate parity (<code>fprp</code>): calculated as FP / (TN
      + FP), quantifes whether the false positive rate is the same across
      groups.</p></li>
<li><p>True positive rate parity (<code>tprp</code>): calculated as TP / (TP +
      FN), quantifies whether the true positive rate is the same across
      groups.</p></li>
<li><p>Negative predicted value parity (<code>npvp</code>): calculated as TN /
      (TN + FN), quantifies whether the negative predicted value is equal
      across groups.</p></li>
<li><p>Specificity parity (<code>sp</code>): calculated as TN / (TN + FP),
      quantifies whether the true positive rate is the same across groups.</p></li>
<li><p>Demographic parity (<code>dp</code>): calculated as TP + FP, quantifies
      whether the positive predictions are equal across groups.</p></li>
</ul><p>Note that, in an audit context, not all fairness measures are equally
  appropriate in all situations. The fairness tree below aids in choosing
  which fairness measure is appropriate for the situation at hand (Büyük,
  2023).</p>
<p><img src="figures/fairness-tree.png" width="100%" alt="fairness-tree"></p>
    </div>
    <div id="references">
    <h2>References</h2>
    <p>Büyük, S. (2023). <em>Automatic Fairness Criteria and Fair
  Model Selection for Critical ML Tasks</em>, Master Thesis, Utrecht University.</p>
<p>Calders, T., &amp; Verwer, S. (2010). Three naive Bayes approaches
  for discrimination-free classification. In <em>Data Mining and Knowledge
  Discovery</em>. Springer Science and Business Media LLC.
  <a href="https://doi.org/10.1007/s10618-010-0190-x" class="external-link">doi:10.1007/s10618-010-0190-x</a></p>
<p>Chouldechova, A. (2017). Fair Pprediction with disparate impact:
  A study of bias in recidivism prediction instruments. In <em>Big Data</em>.
  Mary Ann Liebert Inc. <a href="https://doi.org/10.1089/big.2016.0047" class="external-link">doi:10.1089/big.2016.0047</a></p>
<p>Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp;
  Venkatasubramanian, S. (2015). Certifying and removing disparate impact. In
  <em>Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining</em>. <a href="https://doi.org/10.1145/2783258.2783311" class="external-link">doi:10.1145/2783258.2783311</a></p>
<p>Friedler, S. A., Scheidegger, C., Venkatasubramanian, S.,
  Choudhary, S., Hamilton, E. P., &amp; Roth, D. (2019). A comparative study of
  fairness-enhancing interventions in machine learning. In <em>Proceedings
  of the Conference on Fairness, Accountability, and Transparency</em>.
  <a href="https://doi.org/10.1145/3287560.3287589" class="external-link">doi:10.1145/3287560.3287589</a></p>
<p>Fisher, R. A. (1970). <em>Statistical Methods for Research
  Workers</em>. Oliver &amp; Boyd.</p>
<p>Jamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., &amp;
  Wagenmakers, E. J. (2017). Default "Gunel and Dickey" Bayes factors for
  contingency tables. <em>Behavior Research Methods</em>, 49, 638-652.
  <a href="https://doi.org/10.3758/s13428-016-0739-8" class="external-link">doi:10.3758/s13428-016-0739-8</a></p>
<p>Pessach, D. &amp; Shmueli, E. (2022). A review on fairness in machine
  learning. <em>ACM Computing Surveys</em>, 55(3), 1-44. <a href="https://doi.org/10.1145/3494672" class="external-link">doi:10.1145/3494672</a></p>
<p>Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P.
  (2017). Fairness beyond disparate Ttreatment &amp; disparate impact. In
  <em>Proceedings of the 26th International Conference on World Wide Web</em>.
  <a href="https://doi.org/10.1145/3038912.3052660" class="external-link">doi:10.1145/3038912.3052660</a></p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Koen Derks, <a href="mailto:k.derks@nyenrode.nl">k.derks@nyenrode.nl</a></p>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># Frequentist test of specificy parity</span></span></span>
<span class="r-in"><span><span class="fu">model_fairness</span><span class="op">(</span></span></span>
<span class="r-in"><span>  data <span class="op">=</span> <span class="va">compas</span>,</span></span>
<span class="r-in"><span>  sensitive <span class="op">=</span> <span class="st">"Gender"</span>,</span></span>
<span class="r-in"><span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span></span>
<span class="r-in"><span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span></span>
<span class="r-in"><span>  reference <span class="op">=</span> <span class="st">"Male"</span>,</span></span>
<span class="r-in"><span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span></span>
<span class="r-in"><span>  metric <span class="op">=</span> <span class="st">"sp"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 	Classical Algorithmic Fairness Test</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> data: compas</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> metric: specificity parity, reference group: Male, positive class: yes</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> alternative hypothesis: true odds ratio is not equal to 1</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> sample estimates (parity ratio):</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  Female: 1.3203 [1.2794, 1.3568], p-value = &lt; 2.22e-16</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Bayesian test of predictive rate parity</span></span></span>
<span class="r-in"><span><span class="fu">model_fairness</span><span class="op">(</span></span></span>
<span class="r-in"><span>  data <span class="op">=</span> <span class="va">compas</span>,</span></span>
<span class="r-in"><span>  sensitive <span class="op">=</span> <span class="st">"Ethnicity"</span>,</span></span>
<span class="r-in"><span>  target <span class="op">=</span> <span class="st">"TwoYrRecidivism"</span>,</span></span>
<span class="r-in"><span>  predictions <span class="op">=</span> <span class="st">"Predicted"</span>,</span></span>
<span class="r-in"><span>  reference <span class="op">=</span> <span class="st">"Caucasian"</span>,</span></span>
<span class="r-in"><span>  positive <span class="op">=</span> <span class="st">"yes"</span>,</span></span>
<span class="r-in"><span>  metric <span class="op">=</span> <span class="st">"prp"</span>,</span></span>
<span class="r-in"><span>  prior <span class="op">=</span> <span class="cn">TRUE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 	Bayesian Algorithmic Fairness Test</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> data: compas</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> metric: predictive rate parity, reference group: Caucasian, positive class: yes</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> alternative hypothesis: true odds ratio is not equal to 1</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> sample estimates (parity ratio):</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  African_American: 1.1484 [1.1069, 1.1801], BF₁₀ = 231.16</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  Asian: 0.70146 [0.22307, 1.4765], BF₁₀ = 0.024614</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  Hispanic: 0.97266 [0.87625, 1.1469], BF₁₀ = 0.1037</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  Native_American: 0.71814 [0.39033, 1.5174], BF₁₀ = 0.025534</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  Other: 1.0683 [0.85773, 1.2222], BF₁₀ = 0.10106</span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by <a href="https://koenderks.com" class="external-link">Koen Derks</a>.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer></div>

  


  

  </body></html>

