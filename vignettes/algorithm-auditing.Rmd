---
title: "Algorithm auditing: Get started"
author: Koen Derks
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Algorithm auditing: Get started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{jfa}
  %\VignetteKeywords{algorithm, audit, bias, fairness, model, performance}
  %\VignettePackage{jfa}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(jfa)
```

## Introduction

Welcome to the 'Algorithm auditing' vignette of the **jfa** package. Here you
can find a detailed explanation of the functions in the package that facilitate
auditing of algorithms and predictive models. For more detailed explanations of
each function, read the other vignettes on the
[package website](https://koenderks.github.io/jfa/articles/index.html).

## Functions and intended usage

Below you can find an explanation of the available algorithm auditing functions
in **jfa**.

- [`model_fairness()`](#assess-algorithmic-fairness-with-model_fairness)

### Assess algorithmic fairness with `model_fairness()`

[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)

The `model_fairness()` function provides a model-agnotic method to asses bias
in predictive algorithms  by computing various fairness metrics based on the
observed and predicted classes. The function allows for evaluating fairness
using metrics such as demographic parity, proportional parity, predictive rate
parity, accuracy parity, false negative rate parity, false positive rate parity,
true positive rate parity, negative predicted value parity, and specificity
parity. Moreover, the function helps to decide whether groups are treated fairly
to a certain degree and within a certain materiality threshold.

*Full function with default arguments:*

```r
model_fairness(
  data,
  sensitive,
  target,
  predictions,
  reference = NULL,
  positive = NULL,
  metric = c(
    "prp", "pp", "ap", "fnrp", "fprp",
    "tprp", "npvp", "sp", "dp"
  ),
  alternative = c("two.sided", "greater", "less"),
  conf.level = 0.95,
  prior = FALSE
)
```

*Example usage:*

```{r}
model_fairness(
  data = compas,
  sensitive = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  reference = "Caucasian",
  positive = "yes",
  metric = "prp"
)
```

## Benchmarks

To validate the statistical results, **jfa**'s automated
[unit tests](https://github.com/koenderks/jfa/tree/development/tests/testthat)
regularly verify the main output from the package against the following
benchmarks:

- [fairness](https://cran.r-project.org/package=fairness) (R package version 1.2.2)

## References

- Büyük, S. (2023). *Automatic Fairness Criteria and Fair Model Selection for Critical ML Tasks*, Master Thesis, Utrecht University. - [View Online](https://studenttheses.uu.nl/handle/20.500.12932/44225)
- Calders, T., & Verwer, S. (2010). Three naive Bayes approaches for discrimination-free classification. In *Data Mining and Knowledge Discovery*. Springer Science and Business Media LLC. - [View Online](https://doi.org/10.1007/s10618-010-0190-x)
- Chouldechova, A. (2017). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. In *Big Data*. Mary Ann Liebert Inc.  - [View Online](https://doi.org/10.1089/big.2016.0047)
- Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015). Certifying and Removing Disparate Impact. In *Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM. - [View Online](https://doi.org/10.1145/2783258.2783311)
- Fisher, R. A. (1970). *Statistical Methods for Research Workers*. Oliver & Boyd.
- Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., & Roth, D. (2019). A comparative study of fairness-enhancing interventions in machine learning. In *Proceedings of the Conference on Fairness, Accountability, and Transparency*. ACM. - [View Online](https://doi.org/10.1145/3287560.3287589)
- Jamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., & Wagenmakers, E. J. (2017). Default "Gunel and Dickey" Bayes factors for contingency tables. \emph{Behavior Research Methods}, 49, 638-652. - [View Online](https://doi.org/10.3758/s13428-016-0739-8)
- Pessach, D. & Shmueli, E. (2022). A review on fairness in machine learning. *ACM Computing Surveys*, 55(3), 1-44. - [View Online](https://doi.org/10.1145/3494672)
- Zafar, M. B., Valera, I., Gomez Rodriguez, M., & Gummadi, K. P. (2017). Fairness Beyond Disparate Treatment & Disparate Impact. In *Proceedings of the 26th International Conference on World Wide Web*. - [View Online](https://doi.org/10.1145/3038912.3052660)
